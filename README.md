# Open Call: On the Urgency of Governance

Dear humanity (cc: Sam, Ilya),

This is a call for collaboration.
I need you to try to understand my points, make them yours, share them broadly, build on it.

I realize that what I’m about to say may spark a defensive emotional reaction in many readers, making it tempting to dismiss my statements on psychological grounds rather than engaging with the logic behind them.

I’m about to make a statement so bold that I fully understand you might assign around a 0.1% chance it comes from more than a shallow grasp of the subject. However, if you sense the infinite outcome at stake, offering me the benefit of the doubt (and some of your time) is arguably a Pascalian imperative.

## "I can explain how to solve the alignment problem"

I know it doesn’t sound like a valid statement. I realize the sheer scale of “benefit of the doubt” I’m asking you for.

My plan for this repository is to provide written arguments that gradually lead to the statement “alignment can be reduced to governance.” I want to show how every step logically supports that conclusion.

---

Here, I’ll offer a TL;DR alongside relevant links (in the future). If I say “I can explain how to solve alignment,” I acknowledge how outrageous that appears. From your current perspective, it likely seems to reflect only a shallow understanding of the field.

I’m fully aware that granting credibility to me at this point might feel unwarranted. But if you do choose to dismiss my claims, I only ask you dismiss them on grounds of flawed assumptions or logic—not purely on an assumption of personal inadequacy.

---

## AI Safety’s Ontology

1. Often, “AI safety” is framed as building a system that is intrinsically “safe.” However, I argue we should focus on what timeline or trajectory that system leads humanity toward. Ultimately, a system’s real-world impact depends not only on its design, but also on how it interacts with social, political, and economic structures.
2. At times, “alignment” is conceptualized in overly simplistic terms—treating it like a chain of isolated causes and effects. Yet the real world behaves more like a metastable ecosystem. This parallels how humans introduced the cane toad in Australia, expecting straightforward outcomes, but missing the complex, entangled feedback loops of an entire habitat.
3. Consequently, the trajectory AGI sets for human society depends on more than the AGI’s internal properties. It arises from the interplay between the system and its broader environment. Just like the cane toad example shows that unanticipated consequences arise in interconnected ecosystems, an AGI’s true alignment will hinge on governance, context, and feedback loops—not merely code.

---

## Page 3: The Role of External Factors

If we frame alignment as “maximizing the likelihood of an outcome that, if described to you, seems aligned with human values,” then the actual results for humanity/economy/society of the creation of AGI might appear to be determined less by a system’s intrinsic safety mechanisms, and more by the external environment and power structures in which that system operates.

This leads to my next assertion:

> **There is no such thing as a ‘Safe ASI.’**

Let’s assume that in 16 months, Ilya announces they’ve built ASI, it’s perfectly safe.
Given current global politics, is it far-fetched to imagine a government stepping in and asserting national-security-level authority over that ASI ?

Do you trust the people that, in all likelyhood, will decide how to handle that ? Do you trust your state/any state do handle that right ?

Even if we assume the ASI itself is, by all measures, as safe as possible, would a perfectly safe AGI controlled by Donald and Elon feel like an overall "safe situation" to you ?

If you think it's simplistic and far fetched, try thinking through any scenario:\
Here's a more plausible one: OpenAI achieves AGI, automates most of the global economy, and advocates for universal basic income. 

Do you think the current power structures—those making high-level economic decisions currently—would allow that shift without contention ?

Hence:

> **‘AGI can’t be more aligned than the set of rules it exists under.’**

Right now, the reality we’ll align to is defined by the existing power structures.

The system’s behavior will be shaped by the governance structures in which it is embedded.

Would you trust any given government to truly align with universal human moral values?

### Avoiding Violence

Once the economy is fully automated, it becomes harder to justify power based on wealth accumulation. A small percentage of people own the majority of property, businesses, and land. But if money no longer matters, how do we address the perceived unfairness of such ownership?

Some might argue that the fair course of action would be to share all the stuff.

---

##  My Next Steps

I’m not sure how many readers I’ve convinced to continue this far. Over the coming weeks and months, I’ll focus on making my case solid enough for more competent individuals to both agree and build upon it.

Ultimately, here’s what I want to demonstrate:

> **If AGI labs move forward with automating the global economy, they incur a moral imperative to develop global governance structures—tools to enable a post-capitalist economy.**



